{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdac4780",
   "metadata": {},
   "source": [
    "# Part C. Multinomial (Softmax) Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cba1c",
   "metadata": {},
   "source": [
    "##  Teoría\n",
    "## Derivación del gradiente de la función de log-verosimilitud en la regresión Softmax\n",
    "\n",
    "El objetivo es derivar el gradiente de la **log-verosimilitud** para el modelo de **regresión logística multinomial (Softmax)**. Este modelo extiende la regresión logística binaria al caso multiclase, permitiendo modelar la probabilidad de pertenencia a cada una de $K$ clases mutuamente excluyentes.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 0. Planteamiento general del modelo\n",
    "\n",
    "Para un conjunto de datos $\\mathcal{D}=\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n$, donde $x^{(i)} \\in \\mathbb{R}^d$ y $y^{(i)} \\in \\{1,2,\\dots,K\\}$, el modelo Softmax define la probabilidad de que un individuo $i$ pertenezca a la clase $k$ como:\n",
    "\n",
    "$$\n",
    "P_{\\theta}(y=k \\mid x^{(i)}) \\;=\\;\n",
    "\\frac{\\exp\\!\\big(\\theta_k^\\top x^{(i)}\\big)}\n",
    "{\\sum_{j=1}^{K}\\exp\\!\\big(\\theta_j^\\top x^{(i)}\\big)},\n",
    "$$\n",
    "\n",
    "donde $\\theta_k \\in \\mathbb{R}^d$ es el vector de parámetros correspondiente a la clase $k$, y $\\theta=[\\theta_1,\\theta_2,\\dots,\\theta_K]$ agrupa todos los parámetros del modelo.\n",
    "\n",
    "El objetivo del entrenamiento es **maximizar la log-verosimilitud** de los datos bajo este modelo.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 1. Función de log-verosimilitud\n",
    "\n",
    "La verosimilitud del conjunto de datos es:\n",
    "$$\n",
    "L(\\theta) \\;=\\; \\prod_{i=1}^{n} P_{\\theta}\\!\\big(y^{(i)} \\mid x^{(i)}\\big).\n",
    "$$\n",
    "\n",
    "Tomando logaritmos:\n",
    "$$\n",
    "\\ell(\\theta) \\;=\\; \\log L(\\theta)\n",
    "\\;=\\; \\sum_{i=1}^{n} \\log P_{\\theta}\\!\\big(y^{(i)} \\mid x^{(i)}\\big).\n",
    "$$\n",
    "\n",
    "Sustituyendo la forma Softmax:\n",
    "$$\n",
    "\\ell(\\theta) \\;=\\;\n",
    "\\sum_{i=1}^{n} \\log\\!\\left(\n",
    "\\frac{\\exp\\!\\big(\\theta_{y^{(i)}}^\\top x^{(i)}\\big)}\n",
    "{\\sum_{j=1}^{K}\\exp\\!\\big(\\theta_j^\\top x^{(i)}\\big)}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Separando numerador y denominador:\n",
    "$$\n",
    "\\ell(\\theta) \\;=\\;\n",
    "\\underbrace{\\sum_{i=1}^{n}\\theta_{y^{(i)}}^\\top x^{(i)}}_{\\text{(A)}}\n",
    "\\;-\\;\n",
    "\\underbrace{\\sum_{i=1}^{n}\\log\\!\\left(\\sum_{j=1}^{K}\\exp\\!\\big(\\theta_j^\\top x^{(i)}\\big)\\right)}_{\\text{(B)}}.\n",
    "$$\n",
    "\n",
    "Esta forma es conveniente: (A) es lineal en los parámetros y (B) es un log-sum-exp, derivable con la regla de la cadena.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 2. Derivación del término (A)\n",
    "\n",
    "El término (A) involucra únicamente los parámetros de la clase verdadera $y^{(i)}$ de cada observación. Al derivar respecto de $\\theta_\\ell$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{(A)}}{\\partial \\theta_\\ell}\n",
    "\\;=\\;\n",
    "\\sum_{i=1}^{n} \\mathbf{1}\\{\\,y^{(i)}=\\ell\\,\\}\\; x^{(i)}.\n",
    "$$\n",
    "\n",
    "Aquí, $\\mathbf{1}\\{\\,y^{(i)}=\\ell\\,\\}$ vale $1$ si $y^{(i)}=\\ell$, y $0$ en caso contrario.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 3. Derivación del término (B) — Regla de la cadena\n",
    "\n",
    "Defínase\n",
    "$$\n",
    "g_i(\\theta) \\;=\\; \\log\\!\\left(\\sum_{j=1}^{K}\\exp\\!\\big(\\theta_j^\\top x^{(i)}\\big)\\right).\n",
    "$$\n",
    "\n",
    "Aplicando la regla de la cadena:\n",
    "$$\n",
    "\\frac{\\partial g_i}{\\partial \\theta_\\ell}\n",
    "\\;=\\;\n",
    "\\frac{1}{\\sum_{j=1}^{K}\\exp\\!\\big(\\theta_j^\\top x^{(i)}\\big)}\n",
    "\\cdot\n",
    "\\exp\\!\\big(\\theta_\\ell^\\top x^{(i)}\\big)\\; x^{(i)}\n",
    "\\;=\\;\n",
    "P_{\\theta}(y=\\ell \\mid x^{(i)})\\; x^{(i)}.\n",
    "$$\n",
    "\n",
    "Sumando sobre $i$ y recordando que (B) **se resta** en $\\ell(\\theta)$:\n",
    "$$\n",
    "-\\frac{\\partial \\text{(B)}}{\\partial \\theta_\\ell}\n",
    "\\;=\\;\n",
    "-\\sum_{i=1}^{n} P_{\\theta}(y=\\ell \\mid x^{(i)})\\; x^{(i)}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 4. Combinación de ambos resultados\n",
    "\n",
    "El gradiente total respecto de $\\theta_\\ell$ resulta:\n",
    "$$\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_\\ell}\n",
    "\\;=\\;\n",
    "\\sum_{i=1}^{n}\n",
    "\\Big(\n",
    "\\mathbf{1}\\{\\,y^{(i)}=\\ell\\,\\} \\;-\\; P_{\\theta}(y=\\ell \\mid x^{(i)})\n",
    "\\Big)\\; x^{(i)}.\n",
    "$$\n",
    "\n",
    "O de forma equivalente (más “visual”):\n",
    "$$\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_\\ell}\n",
    "\\;=\\;\n",
    "\\sum_{i=1}^{n}\n",
    "x^{(i)}\n",
    "\\left[\n",
    "\\mathbf{1}\\{\\,y^{(i)}=\\ell\\,\\}\n",
    "-\n",
    "\\frac{\\exp\\!\\big(\\theta_\\ell^\\top x^{(i)}\\big)}\n",
    "{\\sum_{j=1}^{K}\\exp\\!\\big(\\theta_j^\\top x^{(i)}\\big)}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "Este resultado refleja **observado − predicho** (en probabilidad) ponderado por las características $x^{(i)}$.\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 5. Forma matricial\n",
    "\n",
    "Para implementación vectorizada, sea:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{n \\times d}$: matriz de datos (filas $x^{(i)\\top}$),\n",
    "- $Y \\in \\{0,1\\}^{n \\times K}$: codificación one-hot de las etiquetas,\n",
    "- $P = \\mathrm{softmax}(X\\theta) \\in \\mathbb{R}^{n \\times K}$: probabilidades predichas.\n",
    "\n",
    "Entonces:\n",
    "$$\n",
    "\\nabla_{\\theta}\\, \\ell(\\theta) \\;=\\; X^\\top (Y - P).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Paso 6. Interpretación del resultado\n",
    "\n",
    "- $Y - P$ es la diferencia entre lo **observado** y lo **predicho**.\n",
    "- $X^\\top (Y - P)$ acumula esa diferencia ponderada por cada variable explicativa.\n",
    "- Si el modelo predice bien, $Y \\approx P$ y el gradiente tiende a cero (convergencia).\n",
    "\n",
    "En la práctica se **minimiza** la NLL (negativa de la log-verosimilitud), de modo que:\n",
    "$$\n",
    "\\nabla_{\\theta} L_{\\text{NLL}} \\;=\\; X^\\top (P - Y).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Resultado final\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta_\\ell}\n",
    "=\n",
    "\\sum_{i=1}^{n}\n",
    "x^{(i)}\\Big[\n",
    "\\mathbf{1}\\{\\,y^{(i)}=\\ell\\,\\}\n",
    "-\n",
    "P_{\\theta}(y=\\ell \\mid x^{(i)})\n",
    "\\Big],\n",
    "\\qquad\n",
    "\\nabla_{\\theta}\\, \\ell(\\theta) \\;=\\; X^\\top (Y - P).\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02682a93",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
