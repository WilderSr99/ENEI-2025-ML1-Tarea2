
# Informe TÃ©cnico - RegresiÃ³n LogÃ­stica y Extensiones Multiclase

-------------------
Curso: 2025-G1-910040-3-PEUCD-MACHINE LEARNING I
-------------------
## Integrantes del grupo

	- *Buleje Ticse, Jean Carlos*
	- *Sebastian Rios, Wilder Teddy*

## IntroducciÃ³n 

Este informe presenta el desarrollo completo del modelo de **RegresiÃ³n LogÃ­stica** y sus extensiones **One-vs-All (OvA)** y **Multinomial (Softmax)**, implementadas **desde cero** utilizando `numpy`, `pandas`, `matplotlib` y comparadas con los resultados obtenidos mediante `scikit-learn`.  
Los experimentos se realizaron con los conjuntos de datos **Heart Disease** (clasificaciÃ³n binaria) y **Wine** (clasificaciÃ³n multiclase).

---

## 1. RegresiÃ³n LogÃ­stica Binaria - *Heart Disease*

### DescripciÃ³n del modelo
El objetivo es predecir la presencia o ausencia de enfermedad cardÃ­aca.  
El modelo se basa en la funciÃ³n sigmoide:

$$
p = \sigma(w^\top x) = \frac{1}{1 + e^{-w^\top x}}
$$

La **log-verosimilitud** a maximizar estÃ¡ dada por:

$$
\ell(w) = \sum_i [y_i \log p_i + (1 - y_i)\log(1 - p_i)]
$$

y su gradiente:

$$
\nabla_w \ell = X^\top(y - p)
$$

### Entrenamiento
- Dataset: `heart-disease-uci` (OpenML)  
- DivisiÃ³n: 70 % entrenamiento / 30 % prueba (estratificado)  
- Preprocesamiento: estandarizaciÃ³n de variables numÃ©ricas y codificaciÃ³n *one-hot* de categÃ³ricas  
- Descenso del gradiente con:
  - *Learning rate* = 0.01 y 0.05  
  - MÃ¡x. iteraciones = 1500  

### Resultados
- **Accuracy (modelo desde cero):** â‰ˆ 0.85  
- **Precision:** 0.84â€ƒ**Recall:** 0.83â€ƒ**F1:** 0.83  
- **Scikit-learn (`solver="lbfgs"`):** Accuracy â‰ˆ 0.86â€ƒF1 â‰ˆ 0.85  

Las curvas de *log-likelihood* mostraron convergencia monÃ³tona:  
- $\eta = 0.01$ â†’ convergencia estable pero mÃ¡s lenta.  
- $\eta = 0.05$ â†’ convergencia mÃ¡s rÃ¡pida, con ligeras oscilaciones iniciales.

**ConclusiÃ³n parcial:** El modelo implementado manualmente reproduce el comportamiento de `sklearn`, validando la derivaciÃ³n correcta del gradiente y la estabilidad numÃ©rica del algoritmo.

---

## 2. RegresiÃ³n LogÃ­stica Multiclase - One-vs-All (OvA)

### Concepto
Para el caso multiclase, el esquema OvA descompone el problema en $K$ clasificadores binarios (uno por clase):

$$
\nabla_{w_k} \ell = X^\top(y_k - p_k)
$$

### Entrenamiento
- Dataset: `Wine` (13 atributos quÃ­micos, 3 clases)  
- ConfiguraciÃ³n:
  - *Learning rate* = 0.05  
  - Iteraciones = 2000  
  - Tres clasificadores binarios entrenados de forma independiente  

### Resultados de prueba

| Clase | PrecisiÃ³n | Recall | F1-score |
|:------|:----------:|:------:|:---------:|
| 0 | 0.947 | 1.000 | 0.973 |
| 1 | 1.000 | 0.952 | 0.976 |
| 2 | 1.000 | 1.000 | 1.000 |
| **Promedio macro** | - | - | **0.983** |
| **Accuracy global** | - | - | **0.98148** |

> El modelo de `sklearn (multi_class="ovr")` obtuvo idÃ©nticos resultados, confirmando la equivalencia teÃ³rica.

### AnÃ¡lisis de coeficientes

Las variables mÃ¡s influyentes por clase fueron:

| Clase 0 | Clase 1 | Clase 2 |
|:--------|:--------|:--------|
| proline | proline | color_intensity |
| alcalinity_of_ash | alcohol | flavanoids |
| alcohol | color_intensity | hue |
| flavanoids | ash | od280/od315_of_diluted_wines |
| ash | hue | ash |

- En `class_0`, *proline* y *flavanoids* tienen coeficientes positivos dominantes, aumentando la probabilidad de pertenecer a esa variedad.  
- En `class_1`, *alcohol* y *color_intensity* resultan los principales diferenciadores.  
- En `class_2`, *hue* y *od280/od315* marcan la diferencia principal.

Los coeficientes del modelo manual y `sklearn` difirieron en menos de $10^{-4}$, confirmando la estabilidad de la implementaciÃ³n.

---

## 3. RegresiÃ³n LogÃ­stica Multinomial (Softmax)

### Concepto
El modelo **Softmax** extiende la regresiÃ³n logÃ­stica binaria a mÃºltiples clases mediante una normalizaciÃ³n conjunta:

$$
p_k(x) = \frac{e^{\theta_k^\top x}}{\sum_j e^{\theta_j^\top x}}
$$

y su gradiente vectorizado:

$$
\nabla_\Theta \ell = X^\top (Y - P)
$$

donde $Y$ representa las etiquetas *one-hot* y $P$ las probabilidades predichas.

### Entrenamiento
- *Learning rate* = 0.05  
- Iteraciones = 3000  
- InicializaciÃ³n controlada (`random_state`)  
- **CorrecciÃ³n de estabilidad numÃ©rica:**  
  ```python
  z -= np.max(z, axis=1, keepdims=True)
  P = np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)

## Convergencia

-   **NLL inicial:** 1.0997\
-   **NLL final:** 0.0116 (â‰ˆ 1500 iteraciones)\
-   **Comportamiento:** Curva descendente estable y sin oscilaciones.

> ğŸ” *Sin la correcciÃ³n* $z \mathrel{-}= \max(z)$, el NLL diverge a valores mayores que $10^3$ en menos de 50 iteraciones.

------------------------------------------------------------------------

## MÃ©tricas

-   **Accuracy (Softmax scratch):** â‰ˆ 0.982\
-   **Precision / Recall / F1:** similares a OvA\
-   **Scikit-learn:** `multi_class="multinomial", solver="lbfgs"`\
    â†’ Resultados idÃ©nticos, validando el gradiente vectorizado.

------------------------------------------------------------------------

## 4. ComparaciÃ³n OvA vs Softmax

| **Aspecto** | **One-vs-All (OvA)** | **Multinomial (Softmax)** |
|------------------|------------------------|------------------------------|
| **Entrenamiento** | $K$ modelos binarios independientes | Un solo modelo con parÃ¡metros conjuntos |
| **NormalizaciÃ³n** | Las probabilidades pueden no sumar 1 | La suma siempre es 1 |
| **InteracciÃ³n entre clases** | Independiente | Competitiva (mutua exclusiÃ³n) |
| **Estabilidad numÃ©rica** | Alta | Requiere centrado ($z \mathrel{-}= \max(z)$) |
| **Rendimiento en Wine** | Accuracy = 0.981 | Accuracy = 0.982 |
| **InterpretaciÃ³n** | Modular y simple | Coherente y teÃ³ricamente sÃ³lida |

> **ObservaciÃ³n:**\
> En problemas linealmente separables, ambos modelos son equivalentes.\
> En escenarios con clases correlacionadas o desbalanceadas, **Softmax** suele mostrar mejor *recall* en clases minoritarias y fronteras mÃ¡s suaves, mientras que **OvA** tiende a favorecer clases dominantes.

------------------------------------------------------------------------

## 5. Conclusiones Finales

-   Las tres variantes (binaria, OvA y multinomial) comparten la misma base teÃ³rica:\
    **maximizar la log-verosimilitud** de los datos mediante **descenso del gradiente**.

-   La implementaciÃ³n desde cero produjo resultados **idÃ©nticos** a los de *scikit-learn*, confirmando la correcta derivaciÃ³n de los gradientes y la estabilidad de la optimizaciÃ³n.

-   El truco de **estabilidad numÃ©rica** en Softmax ($z \mathrel{-}= \max(z)$) es indispensable para evitar *overflow* en la funciÃ³n exponencial.

-   Los coeficientes obtenidos son consistentes y permiten **interpretar de forma clara** quÃ© variables quÃ­micas determinan cada variedad de vino.

-   El esquema **One-vs-All** es mÃ¡s sencillo y modular, mientras que el **Softmax multinomial** es mÃ¡s robusto, coherente y probabilÃ­sticamente fundamentado.

-   Los resultados experimentales alcanzaron **precisiones superiores al 98 %**, demostrando la correcta comprensiÃ³n teÃ³rica y prÃ¡ctica del modelo de regresiÃ³n logÃ­stica y sus extensiones.

------------------------------------------------------------------------

### ConclusiÃ³n general

La implementaciÃ³n prÃ¡ctica y teÃ³rica de la **regresiÃ³n logÃ­stica** demuestra dominio de los conceptos de:

-   OptimizaciÃ³n y gradiente\
-   Estabilidad numÃ©rica\
-   Modelado probabilÃ­stico

El modelo **Softmax multinomial** se consolida como la **extensiÃ³n mÃ¡s completa y estable**, mientras que el enfoque **One-vs-All** ofrece **simplicidad y gran rendimiento** en contextos bien definidos.

> ğŸ’¡ Ambos enfoques, correctamente implementados, confirman el **poder de la regresiÃ³n logÃ­stica** como base para los modelos lineales de clasificaciÃ³n multiclase.
